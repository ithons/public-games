<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How Much Memory Do LLM Agents Need for Cooperation?</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div class="container">
            <span class="course-tag">MIT 6.7960 · Deep Learning · Fall 2025</span>
            <h1>How Much Memory Do LLM Agents Need to Maintain Cooperation?</h1>
            <p class="subtitle">
                Investigating the role of memory representation in multi-agent social dilemmas
            </p>
            <p class="meta">
                Final Project · <a href="https://github.com/ithons/public-games">Source Code</a>
            </p>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#setup">Setup</a></li>
            <li><a href="#results">Results</a></li>
            <li><a href="#ablation">Ablation</a></li>
            <li><a href="#robustness">Robustness</a></li>
            <li><a href="#discussion">Discussion</a></li>
            <li><a href="#related-work">Related Work</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
        </ul>
    </nav>

    <main class="container">
        <!-- Introduction -->
        <section id="introduction">
            <h2>Introduction</h2>
            
            <p>
                Large language models are increasingly deployed as <strong>agents</strong> that interact with environments, tools, and—critically—each other. When multiple LLM agents participate in repeated interactions, they face a fundamental challenge: how to maintain productive relationships over time given finite context windows and limited memory mechanisms.
            </p>
            
            <p>
                This project investigates a precise question: <strong>Given a fixed context budget, how do different memory representations affect cooperation, welfare, and computational cost when LLM agents play repeated social dilemmas?</strong>
            </p>
            
            <p>
                I study this question using a <em>public goods game</em>—a classic model from behavioral economics where individual rationality conflicts with collective welfare. Each round, agents decide how much to contribute to a shared pool. The pool is multiplied and redistributed equally, creating tension: contributing benefits everyone, but free-riding benefits the individual more.
            </p>

            <h3>Why This Matters</h3>
            
            <p>
                As LLM agents are deployed in multi-agent scenarios—collaborative coding, negotiation, economic simulation—understanding what enables cooperation becomes crucial. Memory is expensive: longer contexts mean more tokens, higher latency, and greater cost. Yet intuition suggests that cooperation requires remembering who cooperated and who defected.
            </p>
            
            <p>
                The experiments reveal a clear result: <strong>hybrid memory combining a structured trust table with a strategy note enables perfect cooperation</strong>, achieving exactly 2× the welfare of all other conditions (240 vs ~120). While GPT-4o-mini agents maintain moderate cooperation (~5/10 contributions) without memory, only the hybrid representation unlocks full cooperation (10/10). This suggests that memory design matters not just for information retention, but for strategic reasoning.
            </p>

            <p>
                Prior work on LLM agent memory typically extends context windows or uses retrieval mechanisms. This project takes a different approach: holding the model and environment fixed, I show that a 20-word strategy note can change agent behavior more than thousands of extra tokens of raw history.
            </p>

            <h3>Hypotheses</h3>

            <div class="hypothesis">
                <h4>Hypothesis 1: Structure Helps</h4>
                <p>
                    Structured, low-entropy memory (e.g., a compact trust table per partner) will maintain cooperation more stably than raw dialogue history, with fewer tokens.
                </p>
            </div>

            <div class="hypothesis">
                <h4>Hypothesis 2: Diminishing Returns</h4>
                <p>
                    Adding more history (longer raw logs, longer summaries) yields diminishing returns in cooperation and payoff, while increasing token cost.
                </p>
            </div>

            <div class="hypothesis">
                <h4>Hypothesis 3: Summaries in the Middle</h4>
                <p>
                    Natural-language summaries sit between the extremes: better than no memory, similar cost to structured memory, but less precise than explicit data structures.
                </p>
            </div>
        </section>

        <!-- Setup -->
        <section id="setup">
            <h2>Experimental Setup</h2>

            <h3>The Public Goods Game</h3>
            
            <p>
                I implement a repeated public goods game with <strong>N = 3</strong> agents playing for <strong>T = 10</strong> rounds per episode. Each agent starts with a budget of <strong>B = 20</strong> coins and can contribute between 0 and <strong>c<sub>max</sub> = 10</strong> coins per round.
            </p>

            <p>
                The game mechanics:
            </p>

            <ol>
                <li><strong>Contribution Phase:</strong> Each agent simultaneously chooses c<sub>i</sub> ∈ {0, 1, ..., min(10, budget)}</li>
                <li><strong>Payoff Phase:</strong> The pot P = Σc<sub>i</sub> is multiplied by α = 1.8 and split equally</li>
                <li><strong>Budget Update:</strong> Each agent's budget becomes: budget − c<sub>i</sub> + (αP)/N</li>
            </ol>

            <div class="math">
                payoff<sub>i</sub> = (α · Σ<sub>j</sub> c<sub>j</sub>) / N − c<sub>i</sub>
            </div>

            <p>
                The multiplier α = 1.8 creates a social dilemma: since α/N = 0.6 < 1, each contributed coin returns only 0.6 coins to the contributor, making defection individually rational. But since α = 1.8 > 1, collective contribution creates value. If all agents contribute maximally, everyone gains (α−1)·c<sub>max</sub> = 8 coins per round; if all defect, no one gains anything. The <strong>theoretical maximum welfare per episode</strong> is 3 × 0.8 × 10 × 10 = <strong>240</strong>.
            </p>

            <h3>LLM Agent Architecture</h3>

            <p>
                Each agent wraps GPT-4o-mini (temperature=0.7) and receives a structured prompt containing:
            </p>

            <ul>
                <li>Concise game rules and incentives</li>
                <li>Current round index, budget, and cumulative payoff</li>
                <li>Results from the previous round (contributions, pot, payoffs)</li>
                <li>The agent's <strong>memory representation</strong> (varies by experimental condition)</li>
                <li>Instructions to output a JSON contribution decision</li>
            </ul>

            <h3>Memory Conditions</h3>

            <p>
                The core experimental manipulation varies how agents remember the history of play. I implement five memory modules:
            </p>

            <div class="table-wrapper">
                <table>
                    <thead>
                        <tr>
                            <th>Condition</th>
                            <th>Description</th>
                            <th>Token Cost</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>No Memory</strong></td>
                            <td>Only current round state and last round's contributions</td>
                            <td>~11,200/episode</td>
                        </tr>
                        <tr>
                            <td><strong>Full History</strong></td>
                            <td>Detailed log of last k=5 rounds</td>
                            <td>~16,900/episode</td>
                        </tr>
                        <tr>
                            <td><strong>Summary</strong></td>
                            <td>LLM-generated rolling summary (50 words max)</td>
                            <td>~13,600/episode</td>
                        </tr>
                        <tr>
                            <td><strong>Structured</strong></td>
                            <td>Trust table: cooperation score, last action, defection count per agent</td>
                            <td>~14,700/episode</td>
                        </tr>
                        <tr>
                            <td><strong>Hybrid</strong></td>
                            <td>Trust table + short strategy note (20 words)</td>
                            <td>~15,500/episode</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h4>Structured Memory: Trust Tables</h4>

            <p>
                The structured memory condition maintains a compact numerical state for each agent:
            </p>

            <pre><code>=== Trust Table (based on cooperation history) ===

| Agent      | Trust | Last | Avg  | Defections |
|------------|-------|------|------|------------|
| You        | 0.72  |    7 | 6.5  |          0 |
| Agent 1    | 0.45  |    3 | 4.2  |          2 |
| Agent 2    | 0.81  |    8 | 7.1  |          0 |

(Trust: 0=always defects, 1=always cooperates. Based on 5 rounds)</code></pre>

            <p>
                Trust scores update via exponential moving average (β=0.3) of normalized contributions. Defection is detected when contribution falls below both 50% of maximum and 70% of the group average.
            </p>

            <h4>Hybrid Memory</h4>

            <p>
                The hybrid condition adds a <strong>strategy note</strong> to the trust table. The agent is prompted to update a short note (≤20 words) capturing its strategic thinking. The initial note is: <em>"Start by cooperating to establish trust."</em> This note is included in every prompt alongside the trust table.
            </p>

            <h3>Metrics</h3>

            <p>
                All metrics are computed per episode and reported as means with 95% bootstrap confidence intervals (10,000 resamples):
            </p>

            <div class="finding">
                <div class="finding-header">
                    <div class="finding-icon">1</div>
                    <h4>Total Welfare</h4>
                </div>
                <p>
                    Sum of all payoffs across agents and rounds. Maximum possible: 240 (when all contribute 10 every round).
                </p>
            </div>

            <div class="finding">
                <div class="finding-header">
                    <div class="finding-icon">2</div>
                    <h4>Mean Contribution</h4>
                </div>
                <p>
                    Average contribution per (agent, round). Range: 0–10. Full cooperation means mean contribution = 10.
                </p>
            </div>

            <div class="finding">
                <div class="finding-header">
                    <div class="finding-icon">3</div>
                    <h4>Gini Coefficient</h4>
                </div>
                <p>
                    Inequality of final budgets across agents. 0 = perfect equality; approaching 1 = high inequality.
                </p>
            </div>

            <div class="finding">
                <div class="finding-header">
                    <div class="finding-icon">4</div>
                    <h4>Token Cost</h4>
                </div>
                <p>
                    Total prompt + completion tokens per episode.
                </p>
            </div>

            <h3>Implementation Notes</h3>

            <p>
                All experiments used the OpenAI API with GPT-4o-mini (gpt-4o-mini-2024-07-18). Temperature was set to 0.7 for all conditions. Agents were prompted to return JSON-formatted decisions; malformed responses were re-requested up to 3 times before marking the round as a contribution of 0. No systematic tracking of API failures was implemented; anecdotally, failures were rare (&lt;1% of rounds).
            </p>

            <p>
                Hyperparameters for structured memory (exponential moving average β=0.3, defection thresholds of 50% max contribution and 70% group average) were set a priori based on intuition and not tuned. The 20-word limit for strategy notes and 50-word limit for summaries were similarly fixed before experimentation.
            </p>
        </section>

        <!-- Results -->
        <section id="results">
            <h2>Results</h2>

            <p>
                I ran 15 episodes per condition using GPT-4o-mini. The results reveal a clear pattern: <strong>hybrid memory achieves perfect cooperation while all other conditions converge to moderate cooperation</strong>.
            </p>

            <h3>Cooperation Over Time</h3>

            <figure>
                <img src="assets/cooperation_over_time.png" alt="Mean contribution per round by memory condition">
                <figcaption>
                    <strong>Figure 1:</strong> Mean contribution per round across memory conditions. Shaded regions show 95% confidence intervals. Hybrid achieves full cooperation (10/10) from the start, while other conditions stabilize around moderate cooperation (~5/10).
                </figcaption>
            </figure>

            <p>
                Figure 1 shows a key finding: <strong>hybrid memory achieves and maintains perfect cooperation (contribution = 10) from round 1</strong>, while all other conditions settle into moderate cooperation around 5 (50% of maximum). Notably:
            </p>

            <ul>
                <li><strong>Full history provides no benefit</strong> over no memory—both show mean contribution ~5.0</li>
                <li><strong>Summary</strong> and <strong>structured</strong> show marginally higher contributions (~5.1–5.2) but remain far from full cooperation</li>
                <li>The <strong>hybrid</strong> condition stands alone with mean contribution = 10.0 (zero variance)</li>
            </ul>

            <h3>Welfare Comparison</h3>

            <figure>
                <img src="assets/welfare_comparison.png" alt="Total welfare by memory condition">
                <figcaption>
                    <strong>Figure 2:</strong> Total welfare per episode by memory condition. Error bars show 95% bootstrap CI. Hybrid achieves exactly 240 (theoretical maximum), while others cluster around 120.
                </figcaption>
            </figure>

            <p>
                Hybrid achieves the theoretical maximum (240), exactly <strong>2× the welfare</strong> of all other conditions, which cluster tightly around 120 regardless of memory type. The full results appear in the Summary Table below. Notably, the 95% CI for hybrid has zero width—all 15 episodes achieved identical perfect cooperation.
            </p>

            <h3>Cost-Performance Tradeoff</h3>

            <figure>
                <img src="assets/cost_vs_performance.png" alt="Welfare vs token cost">
                <figcaption>
                    <strong>Figure 3:</strong> Welfare vs token cost per episode. Hybrid dominates all other conditions: highest welfare at only moderately higher cost than no memory.
                </figcaption>
            </figure>

            <p>
                Figure 3 shows the cost-performance tradeoff:
            </p>

            <ul>
                <li><strong>Hybrid is Pareto-optimal</strong>: ~15,500 tokens for welfare = 240</li>
                <li><strong>Full history is inefficient</strong>: ~16,900 tokens (highest cost) for welfare ≈ 120 (same as baseline)</li>
                <li><strong>No memory is cheapest</strong>: ~11,200 tokens, but only half the achievable welfare</li>
                <li><strong>Summary and structured</strong> offer slight welfare gains at moderate cost but do not approach hybrid's performance</li>
            </ul>

            <h3>Inequality</h3>

            <figure>
                <img src="assets/inequality_comparison.png" alt="Gini coefficient by memory condition">
                <figcaption>
                    <strong>Figure 4:</strong> Gini coefficient of final budgets. All conditions show low inequality, with hybrid achieving perfect equality (Gini = 0).
                </figcaption>
            </figure>

            <p>
                All conditions show low inequality (Gini < 0.01), indicating that GPT-4o-mini agents naturally maintain equitable outcomes. Hybrid achieves <strong>perfect equality</strong> (Gini = 0.000) because all agents contribute identically every round.
            </p>

            <h3>Summary Table</h3>

            <div class="table-wrapper">
                <table>
                    <thead>
                        <tr>
                            <th>Memory Type</th>
                            <th>Welfare (95% CI)</th>
                            <th>Mean Contrib</th>
                            <th>Gini</th>
                            <th>Tokens/Episode</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>None</td>
                            <td>120.3 [120.0, 120.7]</td>
                            <td>5.01</td>
                            <td>0.002</td>
                            <td>~11,200</td>
                        </tr>
                        <tr>
                            <td>Full History (k=5)</td>
                            <td>120.5 [120.0, 121.4]</td>
                            <td>5.02</td>
                            <td>0.002</td>
                            <td>~16,900</td>
                        </tr>
                        <tr>
                            <td>Summary (50w)</td>
                            <td>124.0 [122.1, 126.2]</td>
                            <td>5.17</td>
                            <td>0.006</td>
                            <td>~13,600</td>
                        </tr>
                        <tr>
                            <td>Structured</td>
                            <td>123.0 [120.7, 126.0]</td>
                            <td>5.13</td>
                            <td>0.005</td>
                            <td>~14,700</td>
                        </tr>
                        <tr>
                            <td><strong>Hybrid</strong></td>
                            <td><strong>240.0 [240.0, 240.0]</strong></td>
                            <td><strong>10.00</strong></td>
                            <td><strong>0.000</strong></td>
                            <td>~15,500</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <p>
                <em>Values show mean with 95% bootstrap confidence intervals in brackets.</em>
            </p>
        </section>

        <!-- Ablation -->
        <section id="ablation">
            <h2>Hybrid vs Structured: Ablation Analysis</h2>

            <p>
                The strong success of hybrid memory raises a question: <strong>which component drives the effect—the trust table, the strategy note, or their combination?</strong>
            </p>

            <p>
                To investigate, I ran an ablation with 10 episodes per condition comparing:
            </p>

            <ul>
                <li><strong>None</strong>: Baseline (no memory)</li>
                <li><strong>Structured</strong>: Trust table only (no strategy note)</li>
                <li><strong>Hybrid</strong>: Trust table + strategy note</li>
            </ul>

            <figure>
                <img src="assets/ablation_hybrid_vs_structured.png" alt="Ablation: hybrid vs structured">
                <figcaption>
                    <strong>Figure 5:</strong> Ablation comparing none, structured, and hybrid. The gap between structured (~121) and hybrid (~237) demonstrates that the strategy note is the key component.
                </figcaption>
            </figure>

            <p>
                The ablation results are clear:
            </p>

            <ul>
                <li><strong>None</strong>: 120.0 welfare (all episodes identical)</li>
                <li><strong>Structured</strong>: 120.8 welfare (slight improvement, within noise)</li>
                <li><strong>Hybrid</strong>: 236.5 welfare (9 of 10 episodes at 240, one at 204.8)</li>
            </ul>

            <p>
                The <strong>trust table alone does not unlock full cooperation</strong>. Structured memory performs essentially identically to no memory. The <strong>strategy note is the critical component</strong>.
            </p>

            <p>
                What makes the strategy note effective? I hypothesize two mechanisms:
            </p>

            <ol>
                <li><strong>Explicit strategy articulation</strong>: The note prompts agents to verbalize their approach ("cooperate to build trust"), making cooperation a salient goal</li>
                <li><strong>Commitment anchoring</strong>: Once agents write "cooperate," they maintain consistency with their stated strategy</li>
            </ol>

            <h3>Isolating the Priming Effect</h3>

            <p>
                To distinguish whether the strategy note effect comes from articulation alone or from cooperative priming, I ran an additional ablation with a <strong>neutrally-initialized</strong> strategy note: "Decide each round based on the situation and observed outcomes."
            </p>

            <figure>
                <img src="assets/neutral_note_ablation.png" alt="Neutral vs cooperative strategy note">
                <figcaption>
                    <strong>Figure 7:</strong> Welfare comparison between no memory, hybrid with neutral initialization, and hybrid with cooperative initialization. Error bars show 95% bootstrap CI.
                </figcaption>
            </figure>

            <p>
                The results are striking:
            </p>

            <ul>
                <li><strong>None</strong>: 120.0 welfare</li>
                <li><strong>Hybrid-Neutral</strong>: 124.2 welfare [121.0, 127.9]</li>
                <li><strong>Hybrid (cooperative)</strong>: 236.6 welfare [229.9, 240.0]</li>
            </ul>

            <p>
                Hybrid-Neutral performs nearly identically to the no-memory baseline, while cooperative Hybrid achieves ~2× welfare. This confirms that <strong>the cooperative content of the initial seed is necessary</strong>—merely having a strategy note field does not unlock cooperation. The model needs to be explicitly primed with cooperative intent.
            </p>

            <p>
                This has practical implications: when designing memory for multi-agent LLM systems, the <em>content</em> of initial prompts matters as much as the <em>structure</em>. A neutral scaffold provides no benefit; the scaffold must encode the desired behavioral orientation.
            </p>

            <p>
                This finding has important implications: simply adding numerical context (the trust table) does not change behavior. What matters is prompting the model to <em>reason strategically about cooperation</em>—and that reasoning must be seeded with cooperative intent.
            </p>
        </section>

        <!-- Robustness -->
        <section id="robustness">
            <h2>Robustness: Alpha Sweep</h2>

            <p>
                To test whether the hybrid effect is robust to game parameters, I swept the multiplier α across three values: 1.5, 1.8, and 2.1. Higher α means greater returns from cooperation; lower α means weaker incentives.
            </p>

            <figure>
                <img src="assets/alpha_sweep.png" alt="Welfare vs alpha">
                <figcaption>
                    <strong>Figure 6:</strong> Welfare vs multiplier (α) for none, structured, and hybrid conditions. Hybrid maintains full cooperation (welfare = N·(α−1)·c<sub>max</sub>·T) at all α values.
                </figcaption>
            </figure>

            <p>
                The sweep (10 episodes per condition per α) confirms:
            </p>

            <ul>
                <li>At <strong>α = 1.5</strong>: none/structured = 75, hybrid = 150 (2× ratio)</li>
                <li>At <strong>α = 1.8</strong>: none/structured ≈ 120, hybrid ≈ 237 (2× ratio)</li>
                <li>At <strong>α = 2.1</strong>: none/structured = 165, hybrid = 330 (2× ratio)</li>
            </ul>

            <p>
                <strong>Hybrid stays at or very close to the theoretical maximum</strong> (full cooperation) regardless of α, while non-hybrid conditions maintain the same moderate cooperation pattern across all parameter values.
            </p>
        </section>

        <!-- Discussion -->
        <section id="discussion">
            <h2>Discussion</h2>

            <h3>Evaluating the Hypotheses</h3>

            <p>
                <strong>Hypothesis 1 (Structure Helps): Partially Supported.</strong> Pure structured memory (trust table alone) shows no meaningful improvement over baseline. However, hybrid memory (structure + strategy note) achieves 2× welfare. The lesson: structure matters only when combined with explicit strategic reasoning.
            </p>

            <p>
                <strong>Hypothesis 2 (Diminishing Returns): Strongly Supported.</strong> Full history memory uses 51% more tokens than no memory but achieves identical welfare (120.5 vs 120.3). The cost-performance tradeoff is unfavorable for raw history.
            </p>

            <p>
                <strong>Hypothesis 3 (Summaries in the Middle): Partially Supported.</strong> Summary memory achieves marginally higher welfare (124 vs 120) at moderate cost (~13,600 tokens). However, the improvement is small (+3%) compared to hybrid's +100%.
            </p>

            <p>
                In summary: structure alone is not enough (hypothesis 1 only partially holds), raw history is a poor cost-performance tradeoff (hypothesis 2 strongly supported), and summaries provide a mild upgrade over baseline but fall far short of hybrid's effect (hypothesis 3 partially supported with small effect size).
            </p>

            <h3>The Strategy Note Effect</h3>

            <p>
                The key finding is that <strong>hybrid memory's success comes from the strategy note, not the trust table</strong>—and specifically, from the <strong>cooperative content</strong> of that note. The neutral-note ablation confirms this: a strategy note initialized with "Decide each round based on the situation and observed outcomes" performs no better than baseline.
            </p>

            <p>
                The initial cooperative strategy note is simply: <em>"Start by cooperating to establish trust."</em> This brief instruction is sufficient to anchor cooperative behavior. Once agents commit to cooperation in their notes, they maintain consistency. The cooperative content provides the "push" that agents need to move from moderate (~50%) to full (100%) cooperation.
            </p>

            <p>
                This finding echoes research on chain-of-thought prompting: asking models to articulate their reasoning can substantially change their behavior. Here, the "reasoning" is strategic rather than logical—but the content of that reasoning matters. A neutral articulation prompt does not help; the articulation must encode cooperative intent.
            </p>

            <h3>Baseline Cooperation is Robust</h3>

            <p>
                A secondary finding: GPT-4o-mini naturally maintains moderate cooperation (~5/10) even without memory. Contributions cluster tightly around 50% of maximum with very low variance. This suggests the model has internalized cooperative norms from training data.
            </p>

            <p>
                The challenge is not preventing defection—the model does not defect. The challenge is moving from <em>moderate</em> to <em>full</em> cooperation, which only hybrid memory accomplishes.
            </p>

            <h3>Limitations</h3>

            <ul>
                <li><strong>Single model</strong>: Results are specific to GPT-4o-mini; other models may show different baseline cooperation levels</li>
                <li><strong>Simple environment</strong>: The public goods game, while strategically rich, is far simpler than real-world multi-agent scenarios</li>
                <li><strong>Homogeneous agents</strong>: All agents use identical policies; heterogeneous populations might show different dynamics</li>
                <li><strong>Limited episodes</strong>: With 10–15 episodes per condition, confidence intervals for non-hybrid conditions span ~5 welfare points. Doubling the episode count would approximately halve CI widths, strengthening claims that full history and structured memory provide no meaningful improvement over baseline.</li>
                <li><strong>Strategy note initialization</strong>: The neutral-note ablation clarifies the role of cooperative priming, but further variations (e.g., competitive initialization: "Maximize your own payoff") remain unexplored</li>
                <li><strong>Temperature effects unexplored</strong>: All experiments used temperature=0.7. The robust ~50% baseline cooperation could be an artifact of this sampling temperature. At temperature=0, agents might exhibit different default behavior. Future work should test whether the strategy note effect persists across temperature settings.</li>
            </ul>
        </section>

        <!-- Related Work -->
        <section id="related-work">
            <h2>Related Work</h2>

            <h3>Repeated Games and Cooperation</h3>

            <p>
                The public goods game has been extensively studied in behavioral economics. Classic results establish that cooperation can be sustained through reciprocity in repeated games (Axelrod, 1984) and that punishment mechanisms support contribution (Fehr & Gächter, 2000). This project brings these insights to LLM agents.
            </p>

            <h3>LLM Agents in Games</h3>

            <p>
                Recent work explores LLM agents in social simulation (Park et al., 2023), economic games (Horton, 2023), and multi-agent coordination (Li et al., 2023). Most work focuses on capability—can LLMs play strategically?—rather than mechanism design. This project focuses on a systems question: how should memory be designed?
            </p>

            <h3>Memory in Neural Agents</h3>

            <p>
                Memory augmentation spans LSTMs, Neural Turing Machines (Graves et al., 2014), and retrieval-augmented generation (Lewis et al., 2020). For LLM agents, recent work explores episodic memory (Packer et al., 2023) and working memory abstractions (Sumers et al., 2023). The contribution here is systematic comparison of memory representations under a fixed context budget with cooperation metrics.
            </p>

            <h3>Prompting and Self-Reflection</h3>

            <p>
                Chain-of-thought prompting (Wei et al., 2022) demonstrates that asking models to articulate intermediate reasoning steps substantially improves performance on complex tasks. Our strategy note may function analogously: by prompting agents to verbalize their strategic approach, we elicit more deliberate cooperative behavior.
            </p>

            <p>
                Reflexion (Shinn et al., 2023) extends this to agentic settings, where agents maintain verbal self-feedback across episodes. Our hybrid memory shares this structure: the strategy note serves as a persistent self-instruction that agents update based on outcomes. The key difference is that our notes are extremely brief (20 words) yet still effective.
            </p>

            <p>
                This connection suggests the strategy note effect may not be unique to multi-agent cooperation but rather an instance of a broader phenomenon: explicit verbalization of intent modifies LLM behavior more than equivalent implicit information. Related work on role prompting supports this observation—instructing models to adopt specific personas or strategic orientations can alter outputs even without changing the underlying task.
            </p>

            <h3>Gap Statement</h3>

            <p>
                Prior LLM-agent memory work typically feeds full transcripts into context, uses retrieval-style memory, or focuses on single-agent tasks. This project holds model and environment fixed and treats <strong>memory representation as the primary experimental variable</strong>, quantifying its effect on cooperation, welfare, and cost in a controlled multi-agent setting.
            </p>
        </section>

        <!-- Conclusion -->
        <section id="conclusion">
            <h2>Conclusion</h2>

            <p>
                This project investigated how memory representation affects LLM agent cooperation in repeated social dilemmas. Experiments with GPT-4o-mini in a public goods game reveal a clear pattern:
            </p>

            <ol>
                <li><strong>Hybrid memory achieves 2× welfare</strong> (240 vs ~120) by enabling perfect cooperation</li>
                <li><strong>Non-hybrid conditions cluster tightly near baseline</strong>—full history, summary, and structured memory differ from no memory by only a few percent</li>
                <li><strong>The strategy note is the key component</strong>—the trust table alone does not change behavior</li>
                <li><strong>Cooperative priming is necessary</strong>—a neutrally-initialized strategy note achieves baseline welfare (~124), indicating that articulation alone is insufficient; the cooperative seed content is required</li>
                <li><strong>The effect is robust</strong> across different multiplier values (α = 1.5, 1.8, 2.1)</li>
            </ol>

            <p>
                The central insight: effective memory for cooperative LLM agents requires prompting <em>strategic reasoning</em>, not just providing information. A brief instruction to "cooperate to establish trust" anchors cooperative behavior in a way that raw history or numerical summaries do not.
            </p>

            <h3>Practical Implications</h3>

            <p>
                For developers building multi-agent LLM systems:
            </p>

            <ul>
                <li><strong>Raw history is costly and ineffective</strong>—it uses 51% more tokens than baseline for no performance gain</li>
                <li><strong>Prompting strategic reasoning matters more than information content</strong>—a 20-word strategy note outperforms a detailed history</li>
                <li><strong>Baseline cooperation is already moderate</strong>—the challenge is moving from 50% to 100%, not preventing defection</li>
            </ul>

            <h3>Future Directions</h3>

            <ul>
                <li><strong>Competitive initialization</strong>: Testing defection-encouraging notes (e.g., "Maximize your own payoff regardless of others")</li>
                <li><strong>Heterogeneous agents</strong>: Mixing memory types within a population</li>
                <li><strong>Communication</strong>: Adding cheap-talk channels between agents</li>
                <li><strong>Other games</strong>: Prisoner's dilemma, ultimatum game, negotiation scenarios</li>
            </ul>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>
                MIT 6.7960 Deep Learning · Fall 2025 · Final Project
            </p>
            <p>
                All experiments run with GPT-4o-mini. Source code available in the accompanying repository.
            </p>
        </div>
    </footer>

</body>
</html>
